# -*- coding: utf-8 -*-
"""SENG474_Word2Vec.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/zt55699/IMDB-Sentiment/blob/main/IMDB_Sentiment_Analysis_SENG474.ipynb

# Loading dataset
"""

# Commented out IPython magic to ensure Python compatibility.
# %rm -rf IMDB-Sentiment
!git clone https://github.com/zt55699/IMDB-Sentiment.git
# %cd IMDB-Sentiment/
# %ls

import pandas as pd

# Read data from files 
train_data = pd.read_csv( "labeledTrainData.tsv", header=0, delimiter="\t", quoting=3 )
test_data = pd.read_csv( "testData.tsv", header=0, delimiter="\t", quoting=3 )
unlabeled_train = pd.read_csv( "unlabeledTrainData.tsv", header=0, delimiter="\t", quoting=3 )

print(train_data.head)
print(test_data.head)

"""# Data Cleaning

Gensim preprocessing doc: https://radimrehurek.com/gensim/parsing/preprocessing.html
"""

import gensim.parsing.preprocessing as gp
from gensim.parsing.preprocessing import preprocess_string
from gensim.parsing.preprocessing import remove_stopwords

# Cast words to lower case; remove HTML tags, puctuation, numbers, short words and meaningless stopwords
# Use Porter Stemming e.g. treat "go", "going", and "went" as the same word
# Remove stop words here, which are the noise for later average vector
FILTERS = [lambda x: x.lower(), gp.strip_tags, gp.strip_punctuation, 
           gp.strip_multiple_whitespaces, gp.strip_short , gp.stem_text,
           gp.remove_stopwords, gp.strip_numeric] # maybe not remove number as well

# clean a sentence, return a list of words
def clean_sentence(raw_sentence):
  return preprocess_string(raw_sentence, FILTERS)

r1 = train_data["review"][0]
print("Before: ", r1)
print("After: ", clean_sentence(r1))

"""### Feature selection"""

def clean_df(df):
  count = 0
  dfc = df.copy()
  reviews = dfc["review"]
  for sentence in reviews:
        sentence = clean_sentence(sentence)
        dfc["review"].loc[count] = sentence
        count+=1
  return dfc

df = clean_df(train_data)
print(df["sentiment"][0])

# get the list of most frequent words in both class
from collections import Counter
import itertools

pos_df = df.loc[df['sentiment'] == 1]
neg_df = df.loc[df['sentiment'] == 0]
pos_arr = pos_df["review"].tolist()
neg_arr = neg_df["review"].tolist()

all_words_pos = []
all_words_neg = []

for i in range (len(pos_arr)):
  all_words_pos+=pos_arr[i]
for i in range (len(neg_arr)):
  all_words_neg+=neg_arr[i]

pos_freqs = dict(zip(Counter(all_words_pos).keys(), Counter(all_words_pos).values()))
pos_freqs = dict(sorted(pos_freqs.items(), key=lambda item: item[1], reverse=True))
pos_largest_freqs = dict(itertools.islice(pos_freqs.items(), 15))

neg_freqs = dict(zip(Counter(all_words_neg).keys(), Counter(all_words_neg).values()))
neg_freqs = dict(sorted(neg_freqs.items(), key=lambda item: item[1], reverse=True))
neg_largest_freqs = dict(itertools.islice(neg_freqs.items(), 15))

print("Positive reviews:")
for key, value in pos_largest_freqs.items():
  print(" ",key,":", value)

print("\nNegative reviews:")
for key, value in neg_largest_freqs.items():
  print(" ",key,":", value)

#df_freq.nlargest(10, 'frequency')

# Commented out IPython magic to ensure Python compatibility.
from wordcloud import WordCloud
import matplotlib.pyplot as plt
# %matplotlib inline  

textpos = " ".join(pos_freqs.keys())
textneg = " ".join(neg_freqs.keys())
# Create a word cloud for psitive words

pos_wordcloud = WordCloud(max_font_size=50, max_words=100, background_color="white").generate_from_frequencies(pos_freqs)
neg_wordcloud = WordCloud(max_font_size=50, max_words=100, background_color="white").generate_from_frequencies(neg_freqs)

print("positive")
plt.figure(figsize=(15, 15))
plt.imshow(pos_wordcloud, interpolation="bilinear")
plt.axis("off")
plt.show()

print("negative")
plt.figure(figsize=(15, 15))
plt.imshow(neg_wordcloud, interpolation="bilinear")
plt.axis("off")
plt.show()

reduced_pos = pos_freqs.copy()
reduced_neg = neg_freqs.copy()
keys_to_remove = ['film','movi','thi','wa', 'ar', 'time', 'thei', 'hi', 'ha','end']
for k in keys_to_remove:
  if k in reduced_pos: del reduced_pos[k]
  if k in reduced_neg: del reduced_neg[k]

pos_largest_freqs = dict(itertools.islice(reduced_pos.items(), 10))
neg_largest_freqs = dict(itertools.islice(reduced_neg.items(), 10))

print("Positive reviews:")
for key, value in pos_largest_freqs.items():
  print(" ",key,":", value)

print("\nNegative reviews:")
for key, value in neg_largest_freqs.items():
  print(" ",key,":", value)

"""# Data Pre-processing

** This block is for Word2Vec use. Doc2Vec has its own data pre-processing method in its section

Word2Vec expects single sentences as inputs, each one as a list of words.
"""

from gensim.summarization.textcleaner import split_sentences

# split a review by sentences, return a list of sentences, for each is a list of words
def split_review (raw_review):
  raw_sentences = split_sentences(raw_review)
  clean_sentences = []
  for s in raw_sentences:
    if len(s) > 0:
      clean_sentences.append( clean_sentence(s))
  return clean_sentences

print("Before: ", r1)
print("After: ", split_review(r1))

# prepare input data for Word2Vec (takes couple minutes):
all_sentences = []  

print(f'Parsing {len(train_data["review"])} sentences from training set...')
train_size = len(train_data["review"])
for i in range (0, train_size):
    # report progress
    progress = (i+1)/train_size *100
    if( progress%20 == 0 ):
        print(f'   {progress}%')  
    all_sentences += split_review( train_data["review"][i])

print("Total labeled:", len(all_sentences), "sentences")

#merge with unlabeles data
print(f'Parsing {len(unlabeled_train["review"])} sentences from unlabeled set...')
unlabel_size = len(unlabeled_train["review"])
for i in range (0, unlabel_size):
    # report progress
    progress = (i+1)/unlabel_size *100
    if( progress%20 == 0 ):
        print(f'   {progress}%')  
    all_sentences += split_review(unlabeled_train["review"][i])

print("Total:", len(all_sentences), "sentences")
print(all_sentences[0])

ave_len = sum([len(s) for s in all_sentences])/len(all_sentences)
print("Average sentence length:", ave_len)

"""# Word Embedding

## 1. Word2Vec
"""

# Output messages for training
from gensim.models import word2vec
import logging
import sys

logging.basicConfig(
    format='%(asctime)s [%(levelname)s] %(name)s - %(message)s',
    level=logging.INFO,
    datefmt='%Y-%m-%d %H:%M:%S',
    stream=sys.stdout,
)
log = logging.getLogger('notebook')

# parameters
num_features = 300    # Word vector dimensionality                      
min_word_count = 40   # Minimum word count                        
num_workers = 4       # Number of threads to run in parallel
context = 10          # Context window size                                                                                    
downsampling = 1e-3   # Downsample setting for frequent words

# model training
word2vec_model = word2vec.Word2Vec(all_sentences, workers=num_workers, 
            size=num_features, min_count = min_word_count, 
            window = context, sample = downsampling)

word2vec_model.init_sims(replace=True) # internally calculates unit-length normalized vectors

# save model to drive for later use OPTIONAL
from google.colab import drive
drive.mount('/content/drive')

def save_model(model, name):
  model_save_name = f'{name}({num_features},{min_word_count},{context})'
  path = f"/content/drive/MyDrive/{model_save_name}" 
  model.save(path)

save_model(word2vec_model, "Word2Vec")

"""load saved Word2Vec model"""

# Commented out IPython magic to ensure Python compatibility.
# mount google drive to load pre-saved model
import pandas as pd
import numpy as np
from gensim.models import Word2Vec
from google.colab import drive
drive.mount('/content/drive',  force_remount=True)
# %cd ~/../content/drive/MyDrive
# %ls

# load trained model OPTIONAL
word2vec_model = Word2Vec.load("Word2Vec(300,40,10)")
word2vec_model.trainables.syn1neg.shape

word2vec_model.wv["man"] # word vec

word2vec_model.wv.most_similar("woman")

word2vec_model.wv.most_similar("good")

"""### Word encoding visualization

Visualization of Word Embedding Vectors using Gensim and PCA 
link:https://towardsdatascience.com/visualization-of-word-embedding-vectors-using-gensim-and-pca-8f592a5d3354
"""

X=word2vec_model[word2vec_model.wv.vocab]

df=pd.DataFrame(X)

num_rows = df.shape[0]

df.index = list(word2vec_model.wv.vocab)

df.shape
df.head()

#Computing the correlation matrix
X_corr=df.corr()

#Computing eigen values and eigen vectors
values,vectors=np.linalg.eig(X_corr)

#Sorting the eigen vectors coresponding to eigen values in descending order
args = (-values).argsort()
values = vectors[args]
vectors = vectors[:, args]

#Taking first 2 components which explain maximum variance for projecting
new_vectors=vectors[:,:2]

#Projecting it onto new dimesion with 2 axis
neww_X=np.dot(X,new_vectors)

import matplotlib.pyplot as plt
num_of_words = 1500 #Number of words to draw

plt.figure(figsize=(20,15))
plt.scatter(neww_X[:num_of_words,0],neww_X[:num_of_words,1],linewidths=1,
            c = (neww_X[:num_of_words,0]+neww_X[:num_of_words,1]), cmap = "rainbow")
plt.xlabel("PC1",size=15)
plt.ylabel("PC2",size=15)
plt.title("Word2Vec Embedding Space",size=20)
vocab=list(word2vec_model.wv.vocab)[:num_of_words]

for i, word in enumerate(vocab):
  plt.annotate(word,xy=(neww_X[i,0],neww_X[i,1]))

"""## 2. Doc2Vec

Gensim Doc2Vec doc https://radimrehurek.com/gensim/models/doc2vec.html

Doc2Vec preprocessing
"""

import gensim.utils
from gensim.models.doc2vec import Doc2Vec, TaggedDocument

# prepare input data for Doc2Vec (takes couple minutes):
taggeddoc = []  

print(f'Parsing {len(train_data["review"])} sentences from training set...')
train_size = len(train_data["review"])
for i in range (0, train_size):
    # report progress
    progress = (i+1)/train_size *100
    if( progress%20 == 0 ):
        print(f'   {progress}%')  
    clean_r = clean_sentence( train_data["review"][i])
    taggeddoc.append(TaggedDocument(gensim.utils.to_unicode(str.encode(' '.join(clean_r))).split(),str(i)))

print(f'Parsing {len(unlabeled_train["review"])} sentences from unlabeled_train set...')
unlabeled_train_size = len(unlabeled_train["review"])
for i in range (0, unlabeled_train_size):
    # report progress
    progress = (i+1)/unlabeled_train_size *100
    if( progress%20 == 0 ):
        print(f'   {progress}%')  
    clean_r = clean_sentence( unlabeled_train["review"][i])
    taggeddoc.append(TaggedDocument(gensim.utils.to_unicode(str.encode(' '.join(clean_r))).split(),str(i)))

"""Doc2Vec training"""

import logging
import sys

# output messages
logging.basicConfig(
    format='%(asctime)s [%(levelname)s] %(name)s - %(message)s',
    level=logging.INFO,
    datefmt='%Y-%m-%d %H:%M:%S',
    stream=sys.stdout,
)
log = logging.getLogger('notebook')

# parameters
num_features = 300    # Word vector dimensionality                      
min_word_count = 40   # Minimum word count                        
num_workers = 4       # Number of threads to run in parallel
context = 10          # Context window size - smaller window size gives results that are more syntactic in nature. Larger window ( size > 5 ) gives results that are more semantic - this takes more training time given larger window size.                                                                                   
downsampling = 1e-3   # Downsample setting for frequent words

# model training
doc2vec_model = Doc2Vec(taggeddoc, workers=num_workers, 
                        vector_size= num_features, min_count = min_word_count, 
                        window = context, sample = downsampling)

#save trained Doc2Vec_model to google drive for later use
save_model(doc2vec_model, "Doc2Vec")

doc2vec_model.wv.most_similar("woman")

"""###Word encoding visualization

Visualization of Word Embedding Vectors using Gensim and PCA 
link:https://towardsdatascience.com/visualization-of-word-embedding-vectors-using-gensim-and-pca-8f592a5d3354
"""

X=doc2vec_model[doc2vec_model.wv.vocab]

df=pd.DataFrame(X)

num_rows = df.shape[0]

df.index = list(doc2vec_model.wv.vocab)

df.shape
df.head()

#Computing the correlation matrix
X_corr=df.corr()

#Computing eigen values and eigen vectors
values,vectors=np.linalg.eig(X_corr)

#Sorting the eigen vectors coresponding to eigen values in descending order
args = (-values).argsort()
values = vectors[args]
vectors = vectors[:, args]

#Taking first 2 components which explain maximum variance for projecting
new_vectors=vectors[:,:2]

#Projecting it onto new dimesion with 2 axis
neww_X=np.dot(X,new_vectors)

import matplotlib.pyplot as plt
num_of_words = 1500 #Number of words to draw

plt.figure(figsize=(20,15))
plt.scatter(neww_X[:num_of_words,0],neww_X[:num_of_words,1],linewidths=1,
            c = (neww_X[:num_of_words,0]+neww_X[:num_of_words,1]), cmap = "rainbow")
plt.xlabel("PC1",size=15)
plt.ylabel("PC2",size=15)
plt.title("Doc2Vec Embedding Space",size=20)
vocab=list(doc2vec_model.wv.vocab)[:num_of_words]

for i, word in enumerate(vocab):
  plt.annotate(word,xy=(neww_X[i,0],neww_X[i,1]))

"""##Window Size Exploration

The Gensim default window size is 5 (two words before and two words after the input word, in addition to the input word itself).
"""

def plot_encoding_space(wdmodel, name, pints_to_draw = 1500):
  X=wdmodel[wdmodel.wv.vocab]
  df=pd.DataFrame(X)
  num_rows = df.shape[0]
  df.index = list(wdmodel.wv.vocab)

  X_corr=df.corr()
  values,vectors=np.linalg.eig(X_corr)
  args = (-values).argsort()
  values = vectors[args]
  vectors = vectors[:, args]


  new_vectors=vectors[:,:2]
  neww_X=np.dot(X,new_vectors)

  plt.figure(figsize=(20,15))
  plt.scatter(neww_X[:pints_to_draw,0],neww_X[:pints_to_draw,1],linewidths=1,
              c = (neww_X[:pints_to_draw,0]+neww_X[:pints_to_draw,1]), cmap = "rainbow")
  plt.xlabel("PC1",size=15)
  plt.ylabel("PC2",size=15)
  plt.title(name,size=20)
  vocab=list(wdmodel.wv.vocab)[:pints_to_draw]

  for i, word in enumerate(vocab):
    plt.annotate(word,xy=(neww_X[i,0],neww_X[i,1]))

win_sizes = np.array([1,5,10,15,20,25])
word2vec_models = [word2vec.Word2Vec(all_sentences, workers=num_workers, 
                                      size=num_features, min_count = min_word_count, 
                                      window = win_s, sample = downsampling) 
                  for win_s in win_sizes]

doc2vec_models = [Doc2Vec(taggeddoc, workers=num_workers, 
                          vector_size= num_features, min_count = min_word_count, 
                          window = win_s, sample = downsampling)
                  for win_s in win_sizes]

for i in range(len(win_sizes)):
    plot_encoding_space(word2vec_models[i], "Word2Vec Embedding Space (Win size"+str(win_sizes[i])+")")

for i in range(len(win_sizes)):
    plot_encoding_space(doc2vec_models[i], "Doc2Vec Embedding Space (Win size"+str(win_sizes[i])+")")

"""# Building Feature Set"""

# same clean process apply to input data, keep consistent with the keys in encoding models vc dic
clean_reviews = []

for review in train_data["review"]:
  clean_reviews.append(clean_sentence(review))

"""get the feature set by averaging the word vectors in a single review"""

import numpy as np

# take a list of words as input, return average vector
def get_average_vec(review, model, n_features = num_features):
    vectorized = [model.wv[word] for word in review if word in model.wv.vocab]
    total = len(vectorized)
    sum_v = np.sum(vectorized, axis=0)
    average_v = np.divide(sum_v, total)
    return average_v

def to_feature_set(model, method, reviews):
  f_set = []
  train_size = len(reviews)
  print(f'Processing {train_size} training reviews...')
  
  for i in range (0, train_size):
    # report progress
    progress = (i+1)/train_size *100
    if( progress%20 == 0 ):
        print(f'   {progress}%')  

    avg_v = method(reviews[i], model)
    f_set.append(avg_v)
  return f_set

"""Same preprocessing as word2vec to keep input data format uniform"""

word2vec_train_reviews = to_feature_set(word2vec_model, get_average_vec, clean_reviews)

doc2vec_train_reviews = to_feature_set(doc2vec_model, get_average_vec, clean_reviews)

"""Data preparation for window size tuning"""

word2vec_train_difWins = [to_feature_set(mod, get_average_vec, clean_reviews)
                            for mod in word2vec_models]

doc2vec_train_difWins =[to_feature_set(mod, get_average_vec, clean_reviews)
                            for mod in doc2vec_models]

"""# Classifier Modeling"""

# splitting train test sets; this block only prepare the word encoding models with win size = 5
from sklearn.model_selection import train_test_split

X_train1, X_test1, y_train1, y_test1 = train_test_split(word2vec_train_reviews, train_data["sentiment"], 
                                                    test_size=0.2, random_state=42)

X_train2, X_test2, y_train2, y_test2 = train_test_split(doc2vec_train_reviews, train_data["sentiment"], 
                                                    test_size=0.2, random_state=42)

# this block is for word encoding window size hyterparameter tuning
from sklearn.model_selection import train_test_split


X_trainw1, X_testw1, y_trainw1, y_testw1 = train_test_split(word2vec_train_difWins[0], train_data["sentiment"], 
                                                    test_size=0.2, random_state=42)
X_trainw5, X_testw5, y_trainw5, y_testw5 = train_test_split(word2vec_train_difWins[1], train_data["sentiment"], 
                                                    test_size=0.2, random_state=42)
X_trainw10, X_testw10, y_trainw10, y_testw10 = train_test_split(word2vec_train_difWins[2], train_data["sentiment"], 
                                                    test_size=0.2, random_state=42)
X_trainw15, X_testw15, y_trainw15, y_testw15 = train_test_split(word2vec_train_difWins[3], train_data["sentiment"], 
                                                    test_size=0.2, random_state=42)
X_trainw20, X_testw20, y_trainw20, y_testw20 = train_test_split(word2vec_train_difWins[4], train_data["sentiment"], 
                                                    test_size=0.2, random_state=42)
X_trainw25, X_testw25, y_trainw25, y_testw25 = train_test_split(word2vec_train_difWins[5], train_data["sentiment"], 
                                                    test_size=0.2, random_state=42)

X_traind1, X_testd1, y_traind1, y_testd1 = train_test_split(doc2vec_train_difWins[0], train_data["sentiment"], 
                                                    test_size=0.2, random_state=42)
X_traind5, X_testd5, y_traind5, y_testd5 = train_test_split(doc2vec_train_difWins[1], train_data["sentiment"], 
                                                    test_size=0.2, random_state=42)
X_traind10, X_testd10, y_traind10, y_testd10 = train_test_split(doc2vec_train_difWins[2], train_data["sentiment"], 
                                                    test_size=0.2, random_state=42)
X_traind15, X_testd15, y_traind15, y_testd15 = train_test_split(doc2vec_train_difWins[3], train_data["sentiment"], 
                                                    test_size=0.2, random_state=42)
X_traind20, X_testd20, y_traind20, y_testd20 = train_test_split(doc2vec_train_difWins[4], train_data["sentiment"], 
                                                    test_size=0.2, random_state=42)
X_traind25, X_testd25, y_traind25, y_testd25 = train_test_split(doc2vec_train_difWins[5], train_data["sentiment"], 
                                                    test_size=0.2, random_state=42)

"""## Random forest


"""

from sklearn.ensemble import RandomForestClassifier as rfc

RF = rfc(n_estimators=100)

# train
RF1 = RF.fit(X_train1, y_train1)
print (f'Word2Vec encoding\n  Train accuracy:{RF1.score(X_train1, y_train1)}\n  Test accuracy:{RF1.score(X_test1, y_test1)}')

RF2 = RF.fit(X_train2, y_train2)
print (f'Doc2Vec encoding\n  Train accuracy:{RF2.score(X_train2, y_train2)}\n  Test accuracy:{RF2.score(X_test2, y_test2)}')

# predict
#result = RF1.predict(y_train)

"""### word encoding win size tuning"""

from sklearn.ensemble import RandomForestClassifier as rfc

RF = rfc(n_estimators=100)

RFw_scores = []
RFd_scores = []
# train
RFw1 = RF.fit(X_trainw1, y_trainw1)
RFw_scores.append((RF.score(X_trainw1, y_trainw1),RF.score(X_testw1, y_testw1)))

RFw5 = RF.fit(X_trainw5, y_trainw5)
RFw_scores.append((RF.score(X_trainw5, y_trainw5), RF.score(X_testw5, y_testw5)))

RFw10 = RF.fit(X_trainw10, y_trainw10)
RFw_scores.append((RF.score(X_trainw10, y_trainw10), RF.score(X_testw10, y_testw10)))

RFw15 = RF.fit(X_trainw15, y_trainw15)
RFw_scores.append((RF.score(X_trainw15, y_trainw15),RF.score(X_testw15, y_testw15)))

RFw20 = RF.fit(X_trainw20, y_trainw20)
RFw_scores.append((RF.score(X_trainw20, y_trainw20), RF.score(X_testw20, y_testw20)))

RFw25 = RF.fit(X_trainw25, y_trainw25)
RFw_scores.append((RF.score(X_trainw25, y_trainw25), RF.score(X_testw25, y_testw25)))

RFd1 = RF.fit(X_traind1, y_traind1)
RFd_scores.append((RF.score(X_traind1, y_traind1), RF.score(X_testd1, y_testd1)))

RFd5 = RF.fit(X_traind5, y_traind5)
RFd_scores.append((RF.score(X_traind5, y_traind5), RF.score(X_testd5, y_testd5)))

RFd10 = RF.fit(X_traind10, y_traind10)
RFd_scores.append((RF.score(X_traind10, y_traind10), RF.score(X_testd10, y_testd10)))

RFd15 = RF.fit(X_traind15, y_traind15)
RFd_scores.append((RF.score(X_traind15, y_traind15), RF.score(X_testd15, y_testd15)))

RFd20 = RF.fit(X_traind20, y_traind20)
RFd_scores.append((RF.score(X_traind20, y_traind20), RF.score(X_testd20, y_testd20)))

RFd25 = RF.fit(X_traind25, y_traind25)
RFd_scores.append((RF.score(X_traind25, y_traind25), RF.score(X_testd25, y_testd25)))


print(RFw_scores)
print(RFd_scores)

import matplotlib.pyplot as plt
x = [1,5,10,15,20,25]
yw = [e[1] for e in RFw_scores]
yd = [e[1] for e in RFd_scores]

plt.plot(x, yd, label="doc2vec")
plt.plot(x, yw, label="word2vec")
plt.xlabel('Window size')
plt.ylabel('Test accuracy')
plt.legend()
#plt.title('')
plt.show()

"""## Naive Bayes

link: https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNB
"""

from sklearn.naive_bayes import GaussianNB
bayes_clf = GaussianNB()

bayes_clf1 = bayes_clf.fit(X_train1, y_train1)
print (f'Word2Vec encoding\n  Train accuracy:{bayes_clf1.score(X_train1, y_train1)}\n  Test accuracy:{bayes_clf1.score(X_test1, y_test1)}')

bayes_clf2 = bayes_clf.fit(X_train2, y_train2)
print (f'Doc2Vec encoding\n  Train accuracy:{bayes_clf2.score(X_train2, y_train2)}\n  Test accuracy:{bayes_clf2.score(X_test2, y_test2)}')



"""## SVM

SVM with Linear kernel
"""

# may take an hour for runing this block
from sklearn.svm import SVC

C = 10

svc_clf = SVC(kernel='linear', C=C, probability=True, random_state=0)

svc_clf1 = svc_clf.fit(X_train1, y_train1)
print("Word2Vec encoding Test accuracy:" ,svc_clf1.score(X_test1, y_test1))

svc_clf2 = svc_clf.fit(X_train2, y_train2)
print("Doc2Vec encoding Test accuracy:" ,svc_clf2.score(X_test2, y_test2))

# #Finding the optimum C
# Nc = np.array(range(2, 18) step)

# costs = np.array([best_cost(scaled_2d, i, uniform_random_init) for i in Nc])
# print(costs)
# pl.plot(Nc, costs)
# pl.xlabel('Number of Clusters')
# pl.ylabel('Cost')
# pl.title('Elbow Curve')
# pl.show()

"""SVG with Gaussian kernel"""

svc_clf_g = SVC(kernel="rbf", C=10.0, probability=True, random_state=0)

svc_clf_g1 = svc_clf_g.fit(X_train1, y_train1)
print("Word2Vec encoding Test accuracy:" ,svc_clf_g1.score(X_test1, y_test1))

svc_clf_g2 = svc_clf_g.fit(X_train2, y_train2)
print("Doc2Vec encoding Test accuracy:" ,svc_clf_g2.score(X_test2, y_test2))

"""## Logistic regression

word2vec encoding
"""

from sklearn.linear_model import LogisticRegression
c0=0.00001
alpha=3
cs=[]
for i in range(0, 15, 1):
  cs = np.append(cs,c0*(alpha**i))
print("C parameter: ", cs)

train_errors = list()
test_errors = list()
for c in cs:
    clf = LogisticRegression(C=c, multi_class="multinomial", solver="lbfgs", penalty="l2", max_iter=10000)
    clf.fit(X_train1, y_train1)
    train_errors.append(100*clf.score(X_train1, y_train1))
    test_errors.append(100*clf.score(X_test1, y_test1))

print(train_errors) 
print(test_errors)
i_c_optim = np.argmax(test_errors)
c_optim = cs[i_c_optim]

# Plot results functions

plt.plot(cs, train_errors, label='Train')
plt.plot(cs, test_errors, label='Test')
plt.vlines(c_optim, np.max(test_errors)-1, np.max(test_errors), color='k',
           linewidth=3, label='Optimum on test')
plt.legend(loc='lower right')
plt.ylim([70, 100])
plt.xlabel('Regularization parameter')
plt.ylabel('Accuracy(%)')

plt.show()

print("Optimal regularization parameter : %s" % c_optim)
print("Best train_accuracy: %f  Best test_accuracy: %f" % (np.max(train_errors),np.max(test_errors)))

c0=0.00001
alpha=3
cs=[]
for i in range(0, 15, 1):
  cs = np.append(cs,c0*(alpha**i))
print("C parameter: ", cs)

train_errors = list()
test_errors = list()
for c in cs:
    clf = LogisticRegression(C=c, multi_class="multinomial", solver="lbfgs", penalty="l2", max_iter=10000)
    clf.fit(X_train2, y_train2)
    train_errors.append(100*clf.score(X_train2, y_train2))
    test_errors.append(100*clf.score(X_test2, y_test2))

print(train_errors) 
print(test_errors)
i_c_optim = np.argmax(test_errors)
c_optim = cs[i_c_optim]

# Plot results functions

plt.plot(cs, train_errors, label='Train')
plt.plot(cs, test_errors, label='Test')
plt.vlines(c_optim, np.max(test_errors)-1, np.max(test_errors), color='k',
           linewidth=3, label='Optimum on test')
plt.legend(loc='lower right')
plt.ylim([70, 100])
plt.xlabel('Regularization parameter')
plt.ylabel('Accuracy(%)')

plt.show()

print("Optimal regularization parameter : %s" % c_optim)
print("Best train_accuracy: %f  Best test_accuracy: %f" % (np.max(train_errors),np.max(test_errors)))